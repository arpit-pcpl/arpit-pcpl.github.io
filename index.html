<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Speechmatics Real-Time Transcription with Agent</title>
</head>
<body>
  <h1>Speechmatics Real-Time Transcription</h1>
  <button id="startBtn">Start Transcription</button>
  <button id="stopBtn" disabled>Stop Transcription</button>
  <pre id="transcript"></pre>

  <script>
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const transcript = document.getElementById('transcript');

    let ws;
    let audioContext, processor, input, stream;

    const API_TOKEN = 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzbWlzc3VlciIsImF1ZCI6WyJldSJdLCJleHAiOjE3NDk4MzQ5ODMsIm5iZiI6MTc0OTgzMTMyMywiaWF0IjoxNzQ5ODM0OTIzLCJjb250cmFjdF9pZCI6IjE2NjcxNiIsInByb2plY3RfaWQiOiIxNjY3MTYiLCJwcm9kdWN0IjoiZmxvdyIsInVzZXJfaWQiOiJzZWxmc2VydmljZS0xNjY3MTciLCJhY2NvdW50X3R5cGUiOiJmcmVlIiwiY29ubmVjdGlvbl9xdW90YSI6IjMifQ.EuSOUAHEyEOTNlroH4_qaa-XzaROofi40mnW_DkpZjrMpsYIlwWycSxnPDKRDDO_D1RsMfWWc21PEMJumtfoqGE_P4yi-k8_cChrI-N_3zIr7ty3gqw97XsWWKxVja_6fcFVrV4InxADJhz9p-s_hv0wZrF0ZusoGknvvHN0-nJbproXoB36UbWyK-hdg8ImnprGURc0WWfLs_QdYDqMP7iqyJQv3gIV_89i2rZ-NP99NpbXh94bQx3mMGJ7AaCSgyoZN_jWCdG6sXgIYZLma76Gl2VwddVz8QnJwf8_fEeorX-m3GlhbzRk4SnJ8uzqc8SVoDAV5uxp0yc7jMjLKF7Hw0AcysYc9uOIj_waHcA3CojmHOBzLAw0AdaAv06oUo9Jf-Md8OhQqaNWvus6GKxq3KEKb15VXcuGqBnUGa0YymLhgJYjbgkrbNcv3yWkDeZAiDyjw4NNGx8SCiksyNDzn219kaaa3s_ceHRwMo4lAtP9UASAaU9PidT5R7IB';
    const AGENT_ID = '3668c60a-1c08-4670-81e9-97e12e5a4149:latest'; 
    const LANG = 'en';

    const SM_URL = `wss://eu.rt.speechmatics.com/v2/${LANG}?auth_token=${API_TOKEN}&agent=${AGENT_ID}`;

    startBtn.onclick = async () => {
      startBtn.disabled = true;
      stopBtn.disabled = false;
      transcript.textContent = '';

      stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioContext = new AudioContext();
      input = audioContext.createMediaStreamSource(stream);

      processor = audioContext.createScriptProcessor(4096, 1, 1);
      processor.onaudioprocess = e => {
        const inputData = e.inputBuffer.getChannelData(0);
        const int16Data = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          int16Data[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32767));
        }
        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(int16Data.buffer);
        }
      };

      input.connect(processor);
      processor.connect(audioContext.destination);

      ws = new WebSocket(SM_URL);
      ws.binaryType = 'arraybuffer';

      ws.onopen = () => {
        console.log('WebSocket opened');
        ws.send(JSON.stringify({
          type: 'start',
          transcription_config: {
            language: LANG,
            operating_point: 'enhanced',
            enable_partials: true
          }
        }));
      };

      ws.onmessage = event => {
        const data = JSON.parse(event.data);
        if (data.type === 'transcript') {
          data.channels[0].alternatives.forEach(alt => {
            transcript.textContent += alt.content + ' ';
          });
        }
      };

      ws.onclose = () => console.log('WebSocket closed');
    };

    stopBtn.onclick = () => {
      startBtn.disabled = false;
      stopBtn.disabled = true;

      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'stop' }));
        ws.close();
      }

      if (processor) processor.disconnect();
      if (input) input.disconnect();
      if (audioContext) audioContext.close();
      if (stream) stream.getTracks().forEach(track => track.stop());
    };
  </script>
</body>
</html>
